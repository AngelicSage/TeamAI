# These are projects my team members and I worked together to create

## Documentation

1. Found as much data as we could of AIME, AMC10, and AMC12 math
2. We cleaned and concatenated our code 
3. Decided to work with a 2b version of Gemma and write our own code in tensorflow
4. Then, we decided to go with a 7b quantized version of Gemma, which we used HuggingFace for, so the training code also had to change
5. We looked at our competitors’ code, and copied a notebook from #2 who had a score of 20, Team “Yeah”
6. We added our concatenated data to the code, and then changed the pipeline to suit the new data
